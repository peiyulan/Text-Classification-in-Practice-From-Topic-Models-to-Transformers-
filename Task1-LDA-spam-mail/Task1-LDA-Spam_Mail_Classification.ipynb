{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Text Classification: Topic Modeling LDAâ€‹**\n"
      ],
      "metadata": {
        "id": "5LAarKZlJZBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This course ?????\n",
        "\n",
        "We will learn the ????? through the following activities:\n",
        "\n",
        "- Activity 1:\n",
        "- Activity 2:"
      ],
      "metadata": {
        "id": "3hm6uw_UaSGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OgyNjtzloQ1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview of Latent Dirichlet Allocation (LDA)**"
      ],
      "metadata": {
        "id": "2sH-oL7VMt40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9IWxh3e_gJW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of LDA**\n",
        "\n",
        "- Loading data\n",
        "- Data cleaning\n",
        "- Exploratory analysis\n",
        "- Prepare data for LDA analysis\n",
        "- LDA model training\n",
        "- Analyzing LDA model results\n"
      ],
      "metadata": {
        "id": "xbsjNGskfWhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task Descrption**\n",
        "\n",
        "- Spam mail classification"
      ],
      "metadata": {
        "id": "W8TMNVHng_ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pqsuWVz9gy6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ‘‹ New to Google Colab and Python? No worries! Let's get you started by running the code snippet below to make sure everything is working properly for you. It's a great way to take your first steps into coding!"
      ],
      "metadata": {
        "id": "yjBHVUNPnb7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter your name and press enter:\")\n",
        "name = input()\n",
        "print(\"\\r\")\n",
        "print(\"Hello {}, welcome to the Topic Modelling with LDA!\".format(name))"
      ],
      "metadata": {
        "id": "wsGtYVH4hFg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Python libriary"
      ],
      "metadata": {
        "id": "FR20mbyog1ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "Yiw2ia_hhBHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C-jD-ZQnTcLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"./data/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ],
      "metadata": {
        "id": "vEf8Gb-wg8ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "mvutqiSaQMc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()\n",
        "\n",
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ],
      "metadata": {
        "id": "aVOhXJxdglSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-RhfmjXtOz8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory analysis"
      ],
      "metadata": {
        "id": "Y4g_5lv1ia2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "U1-urCgqia2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Fwy_yf-Lia2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data for LDA analysis"
      ],
      "metadata": {
        "id": "x9ekIE8xiiYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "id": "VbB7SgLUiiYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ],
      "metadata": {
        "id": "dG99StgijrPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dNBjWwHiiiYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train LDA model"
      ],
      "metadata": {
        "id": "t6_rLm0WiifF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "ei7XKRGkiifF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse the trained LDA model"
      ],
      "metadata": {
        "id": "SHjB6NLZiqsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ],
      "metadata": {
        "id": "SCl-cva9jwF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tcthaMLriifG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resources List**\n",
        "\n",
        "**Tutorials**\n",
        "\n",
        "\n",
        "\n",
        "**Readings**\n",
        "\n"
      ],
      "metadata": {
        "id": "Jh1bgAvnxJwx"
      }
    }
  ]
}