{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peiyulan/Text-Classification-in-Practice-From-Topic-Models-to-Transformers-/blob/main/Task1-LDA-News%20Classification/Task1_LDA_News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LAarKZlJZBi"
      },
      "source": [
        "\n",
        "# **Text Classification: Topic Modeling LDAâ€‹**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hm6uw_UaSGQ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "We will learn the about the most widely used text classification model LDA through:\n",
        "\n",
        "> An introduction to what and how does LDA model work?\n",
        "\n",
        "> Implementation of LDA model wiht Python:\n",
        "- Activity 1: Text representation\n",
        "- Activity 2: Training LDA model\n",
        "\n",
        "> Analysising your results\n",
        "- Activity 3: Visualisation of text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgyNjtzloQ1m"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sH-oL7VMt40"
      },
      "source": [
        "# **Overview of Latent Dirichlet Allocation (LDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IWxh3e_gJW-"
      },
      "source": [
        "![LDA.jpg](https://media.springernature.com/full/springer-static/image/chp%3A10.1007%2F978-981-19-3035-5_29/MediaObjects/520032_1_En_29_Fig3_HTML.png?as=webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbsjNGskfWhJ"
      },
      "source": [
        "# **Implementation of LDA**\n",
        "---\n",
        "### Steps\n",
        "- Loading data and visualise data\n",
        "- Prepare data for LDA analysis\n",
        "- Training LDA model\n",
        "- Analysing LDA model results\n",
        "\n",
        "---\n",
        "\n",
        "### Dataset\n",
        "> The datasets we will use today:\n",
        "\n",
        "BBC News Datasets (D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006. [PDF].)\n",
        "\n",
        "The dataset was retrieved from BBC News, and has been widely used in machine-learning and traing text-classification models.\n",
        "These datasets are made available for non-commercial and research purposes only.\n",
        "\n",
        "The dataset contains 2225 news articles published between 2004-2005 on the BBC news website. Each article is labeled with one of the five topics: Politics, Business, Technology, Entertainment and Sport.\n",
        "\n",
        "\n",
        "Source of dataset:\n",
        "https://huggingface.co/datasets/SetFit/bbc-news\n",
        "\n",
        "> CNN-DailyMail News Text Summarization\n",
        "The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n",
        "\n",
        "Source of dataset:\n",
        "https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail?resource=download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqsuWVz9gy6Y"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR20mbyog1ms"
      },
      "source": [
        "# Loading Python library and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yiw2ia_hhBHZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Import Python libriary\n",
        "# ============================\n",
        "\n",
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEf8Gb-wg8ew"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Download datasets\n",
        "# ============================\n",
        "\n",
        "# Open the zip file\n",
        "!wget https://github.com/DCS-training/Text-Calssification-in-Practice-From-Topic-Models-to-Transformers/raw/refs/heads/main/Task1-LDA-News%20Classification/LDA_data.zip\n",
        "!unzip /content/LDA_data.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset to python"
      ],
      "metadata": {
        "id": "iecREVmkh8-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLc0cAjHClyl"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a pandas DataFrame\n",
        "file1 = \"cnnnews_data.csv\"\n",
        "file2 = \"BBCnews_data.csv\"\n",
        "\n",
        "data = pd.read_csv(file1)   # Feel free to change it to file2 and play with different dataset\n",
        "data = data[:2000]          # We pick the first 2000 articles in the dataset for faster results\n",
        "\n",
        "# Print name of each column and numer of data points\n",
        "print(data.info(verbose=True), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 rows in the dataset\n",
        "print(\">>> The first 5 rows in the dataset\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "QtjLrMwuhliT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1-urCgqia2x"
      },
      "outputs": [],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Randomly select 100 articles and join them together (for faster processing)\n",
        "long_string = ','.join(list(data['article'].sample(100).values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwy_yf-Lia2x"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ekIE8xiiYh"
      },
      "source": [
        "# Prepare data for LDA analysis\n",
        "\n",
        "Before we can use the LDA model, data needs to be processed and transformed in a format that can be use as an input for LAD model, these steps inclues:\n",
        "\n",
        "1. Tokenize the text\n",
        "2. Remove stopwords\n",
        "3. Convert the tokenized object into a corpus and dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQF3R5bClym"
      },
      "source": [
        "## 01 - Tokenization\n",
        "\n",
        "Tokenisers work by splitting text into smaller components, or the unit for analysis.\n",
        "\n",
        "For example, an article can be split into sentences, or words, depending on the purpose of your analysis.\n",
        "\n",
        "This is usually done by identifyin the \"separator\"  or \"delimiter\" in the data.\n",
        "\n",
        "For example, the \" \"(blank space) between words in a sentence, or the \",\"(comma) or \".\" (period)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Practice tokenization\n",
        "# ============================\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"This is an example sentence. It has five words.\"#example_sentence\n",
        "print(\"Use sent_tokenize to split data into sentences\")\n",
        "print(sent_tokenize(text)) # nltk.tokenize.sent_tokenize(text, language='english')\n",
        "\n",
        "print(\"\\nUse word_tokenize to split data into sentences\")\n",
        "print(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "MbMD2lOCjsru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjo2MyILClym"
      },
      "outputs": [],
      "source": [
        "# Note that tokenisation is similart but different from .splited list\n",
        "\n",
        "print(word_tokenize(text))\n",
        "print(text.split(\" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, as we are not interested in analysing punctuation, we will first remove punctuation marks and choose \" \" as the diameter to tokenise or data."
      ],
      "metadata": {
        "id": "U22VooVZlckT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfPiOfdnClym"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Remove punctuation\n",
        "# ============================\n",
        "\n",
        "# Load the Regex (regular expression) library to clean up the data\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "article = data['article']\n",
        "cleaned_article = article.map(lambda x: re.sub('[,\\.!?]', '', x)) # Search for each row of data, and replace \"[,\\.!?]\", with \"\" (nothing)\n",
        "cleaned_article = cleaned_article.map(lambda x: x.lower()) # Make everything letter lower-case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcZChMvKClym"
      },
      "outputs": [],
      "source": [
        "# Compare results\n",
        "\n",
        "print(\"Original article:\")\n",
        "print(article[0], \"\\n\")\n",
        "print(\"Article after removing punctuation and lower-case:\")\n",
        "print(cleaned_article[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB-lHchLClym"
      },
      "source": [
        "Now let's use the method above to tokenise the dataset \"news[\"cleaned_article\"]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKUv5FK8Clym"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Tokenize the dataset\n",
        "# ============================\n",
        "\n",
        "tokenised_articles = [word_tokenize(article) for article in cleaned_article]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first tokenised article in the dataset\n",
        "\n",
        "print(tokenised_articles[0])"
      ],
      "metadata": {
        "id": "5w3ttx2fpG8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ayJM9tClym"
      },
      "source": [
        "## 02- Remove stopwords\n",
        "\n",
        "Stop words are commonly used words. In English, for example, \"the\" , \"is\", \"what\", are all stopwords.\n",
        "\n",
        "As these words do not carry much information of meaning in a sentence, they are usually excluded from the data to achieve a better analysis result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrNlg16YClym"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Remove stopwords\n",
        "# ============================\n",
        "\n",
        "import gensim    # open source Python library for text processing\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk      # natual language processing tool kit\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "customised_stopwords = ['from', 'mr', 'edu', 'use'] # You can add your own stopwords\n",
        "stop_words.extend(customised_stopwords)\n",
        "\n",
        "# function for removing stop words\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "tokenised_articles_no_stopwords = remove_stopwords(tokenised_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh9RCaVNClyn"
      },
      "outputs": [],
      "source": [
        "# Compare results\n",
        "\n",
        "print(\"The length of \\nthe article: {}\\ntokenised article: {}\\nwithout stopwords: {}\\n\".format(len(cleaned_article[0]),len(tokenised_articles[0]), len(tokenised_articles_no_stopwords[0])))\n",
        "\n",
        "print(cleaned_article[0], \"\\n\") # Original article\n",
        "print(tokenised_articles[0], \"\\n\") # Tokenised article\n",
        "print(tokenised_articles_no_stopwords[0], \"\\n\") # Tokenised article without stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8iA8eHzClyn"
      },
      "source": [
        "## 03- Text representation: Bag of words (BOW)\n",
        "\n",
        "The last step of text processing is turning textual data into numerical format. In our case, we want to turn a news article into a series of numbers that represent the article.\n",
        "\n",
        "One of the most common way to do this is \"bag-of-words\" or BOW.\n",
        "\n",
        "It works by creating a vocabulary of all unique words in a collection of documents and then representing each document as a vector of word counts.\n",
        "\n",
        "For example:\n",
        "- Tokens: {banana, apple, walk, on, the, street, dog}\n",
        "- Sentence 1 vector: {banana: 2, apple: 1, walk: 1, on: 1, the: 1, street: 1, dog: 1}\n",
        "- Sentence 2 vector: {banana: 1, apple: 0, walk: 1, on: 1, the: 2, street: 1, dog: 0}\n",
        "\n",
        "\n",
        "This represenation ignore the order and grammatical relationship between words, and simply count the number of appearance (frequency) of a given word in the text. It is commonly used in text classification or compare similarity between documents.\n",
        "\n",
        "In general, this model works better on longer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG99StgijrPK"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Create a dictionary of tokens\n",
        "# ============================\n",
        "\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "# This assign an index to each token (word)\n",
        "id2word = corpora.Dictionary(tokenised_articles_no_stopwords)\n",
        "\n",
        "for index in id2word.keys()[:10]:\n",
        "    print(index, id2word[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvBW0x_9Clyn"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Create a corpus (numerical representation of document)\n",
        "# ============================\n",
        "\n",
        "# turn each article into \"Bag of words\"\n",
        "corpus = [id2word.doc2bow(document) for document in tokenised_articles_no_stopwords]\n",
        "\n",
        "# View\n",
        "print(\"article 1\", corpus[0])\n",
        "print(\"article 2\", corpus[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNBjWwHiiiYi"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_rLm0WiifF"
      },
      "source": [
        "## 03 Train LDA model\n",
        "\n",
        "Documentation of the gensim LDA model: https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "\n",
        "Let's try the most basic function using LdaMulticore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei7XKRGkiifF"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# Create a dictionary of tokens\n",
        "# ============================\n",
        "\n",
        "from pprint import pprint\n",
        "import datetime\n",
        "\n",
        "utcnow = datetime.datetime.now(datetime.timezone.utc)\n",
        "\n",
        "\n",
        "# number of topics\n",
        "num_topics = 5\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the keyword for each of the 5 topics\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "OfoGMnM0qcar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHjB6NLZiqsT"
      },
      "source": [
        "# Analyse the trained LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCl-cva9jwF4"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf6jU0cpClyn"
      },
      "source": [
        "## Specify parameters in LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMOEoI2_Clyn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "id2word = corpora.Dictionary(tokenised_articles_no_stopwords)\n",
        "id2word.filter_extremes(\n",
        "#   no_below=5,    # Keep tokens which are in at least 5 documents\n",
        "    no_above=0.5,  # Keep tokens which are in no more than 50% of the documents (fraction)\n",
        "    keep_n=10000  # Keep only the first 100,000 most frequent tokens (optional, for memory control)\n",
        ")\n",
        "corpus = [id2word.doc2bow(document) for document in tokenised_articles_no_stopwords]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "num_topics = 6\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics= num_topics,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=80,\n",
        "                                           per_word_topics=True)\n"
      ],
      "metadata": {
        "id": "SWL5XFFGGD4w",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Print the Keyword in each topic\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "LDAvis_prepared"
      ],
      "metadata": {
        "id": "lGnaC-rxGHSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddQuJ86yClyn"
      },
      "outputs": [],
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "    topic = []\n",
        "    prop= []\n",
        "    keywords = []\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                topic.append(int(topic_num))\n",
        "                prop.append(round(prop_topic,4))\n",
        "                keywords.append(topic_keywords)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df['Dominant_Topic'] = topic\n",
        "    sent_topics_df['Perc_Contribution'] =prop\n",
        "    sent_topics_df['Topic_Keywords']=keywords\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=tokenised_articles_no_stopwords)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "#df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVjNSo93Clyz"
      },
      "outputs": [],
      "source": [
        "df_dominant_topic.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcthaMLriifG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh1bgAvnxJwx"
      },
      "source": [
        "# **Resources List**\n",
        "\n",
        "**Tutorials**\n",
        "Different ways to visualise results of LDA analysis:https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}